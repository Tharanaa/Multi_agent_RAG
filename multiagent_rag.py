# -*- coding: utf-8 -*-
"""Multiagent_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WlwQNtmeggsdcvs8KAsRhCFAW0w4lhxo
"""

#!pip install langchain langgraph cassio

import cassio
#connect to astra DB
ASTRA_DB_APPLICATION_TOKEN="AstraCS:rYCKiLMBWpIakHsfzKjmMzhJ:ef434b458d6c40d81231f13bb88f4ece74f4f2ee81d6aea206b76c328cd2896b"
ASTRA_DB_ID="15bb01aa-d546-4b51-b217-23abfef93320"
cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)

#!pip install langchain_community

#!pip install -U langchain-groq langchainhub chromadb langchain langgraph langchain_huggingface

### Build Index

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader

#Docs to index
urls = [
    "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/"
]

###load
docs=[WebBaseLoader(url).load() for url in urls]
doc_list=[item for sublist in docs for item in sublist]
print(doc_list)
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=0)
docs_split= text_splitter.split_documents(doc_list)

docs_split

from langchain_huggingface import HuggingFaceEmbeddings
embeddings=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

import cassio
#connect to astra DB
ASTRA_DB_APPLICATION_TOKEN="AstraCS:rYCKiLMBWpIakHsfzKjmMzhJ:ef434b458d6c40d81231f13bb88f4ece74f4f2ee81d6aea206b76c328cd2896b"
ASTRA_DB_ID="15bb01aa-d546-4b51-b217-23abfef93320"
cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)

from langchain_community.vectorstores import Cassandra

astra_vector_store=Cassandra(embedding=embeddings,
                             table_name="qa_mini_demo",
                             session=None,
                             keyspace=None)

astra_vector_store.add_documents(docs_split)
print("Inserted %i headlines. ",len(docs_split))

retriever=astra_vector_store.as_retriever()
retriever.invoke("What is agent")

##langraph application
from typing import Literal
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

##data model
class RouteQuery(BaseModel):
  """Route a user query to the most relevant database"""

  datasource:Literal["vectorstore","wiki_search"]=Field(
      description="Given the user question chosen to wikipedia or vectorstore",

  )

from langchain_groq import ChatGroq
#from google.colab import userdata
import os
groq_api_key = os.getenv("GROQ_API_KEY")
#groq_api_key=userdata.get('groq_api_key')
#print(groq_api_key)

llm=ChatGroq(groq_api_key=groq_api_key,model_name="Llama-3.3-70b-Versatile")
llm

structured_llm_router=llm.with_structured_output(RouteQuery)

#prompt
system = """You are an expert at routing a user question to a vectorstore or wikipedia.
The vectorstore contains document related to agents, prompt engineering, and adverserial attacks.
Use the vectorstore for questions on these topics. Otherwise, use wiki-search."""

route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system",system),
        ("human","{question}"),
    ]
)

question_router= route_prompt | structured_llm_router

print(question_router.invoke(
    {
        "question":"What is agent?"
    }
))

print(question_router.invoke(
    {
        "question":"Who is AbdulKalam?"
    }
))

#!pip install langchain_community
#!pip install wikipedia

from langchain_community.utilities import WikipediaAPIWrapper
from langchain_community.tools import WikipediaQueryRun
api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=1000)
wiki=WikipediaQueryRun(api_wrapper=api_wrapper)

wiki.invoke({"query":"Who is AbdulKalam?"})

wiki.run("who is  A P J Abdulkalam")

from typing import List
from typing_extensions import TypedDict
from langchain_core.documents import Document

class GraphState(TypedDict):
  """
  Represents the state of our graph.

  Attributes:
    question:question
    generation: LLM generation
    documents:list of documents
  """
  question:str
  generation:str
  documents:List[Document]

from langchain_core.documents import Document

def retrieve(state):
  """
  Retrieve documents
  Args:
    state(dict): The current graph state
  Returns:
    state(dict): New key added to state, documents, that contains retrieved documents
  """
  print("---Retrieve---")
  question=state["question"]

  ##Retrievel
  documents=retriever.invoke(question)
  return {"documents":documents,"question":question}

def wiki_search(state):
    """
    wiki search based on the re-phrased question.

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Updates documents key with appended web results
    """

    print("---wikipedia---")
    print("---HELLO--")
    question = state["question"]
    print(question)

    # Wiki search
    docs = wiki.invoke({"query": question})

    wiki_results = docs
    wiki_results = Document(page_content=wiki_results)

    return {"documents": wiki_results, "question": question}

### Edges ###


def route_question(state):
    """
    Route question to wiki search or RAG.

    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """

    print("---ROUTE QUESTION---")
    question = state["question"]
    source = question_router.invoke({"question": question})
    if source.datasource == "wiki_search":
        print("---ROUTE QUESTION TO Wiki SEARCH---")
        return "wiki_search"
    elif source.datasource == "vectorstore":
        print("---ROUTE QUESTION TO RAG---")
        return "vectorstore"

from langgraph.graph import END, StateGraph, START

workflow = StateGraph(GraphState)
# Define the nodes
workflow.add_node("wiki_search", wiki_search)  # web search
workflow.add_node("retrieve", retrieve)  # retrieve

# Build graph
workflow.add_conditional_edges(
    START,
    route_question,
    {
        "wiki_search": "wiki_search",
        "vectorstore": "retrieve",
    },
)
workflow.add_edge( "retrieve", END)
workflow.add_edge( "wiki_search", END)
# Compile
app = workflow.compile()

''''from IPython.display import Image, display

try:
    display(Image(app.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass'''

from pprint import pprint

# Run
inputs = {
    "question": "What is agent?"
}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation
pprint(value['documents'][0].dict()['metadata']['description'])

from pprint import pprint

# Run
inputs = {
    "question": "Avengers"
}
for output in app.stream(inputs):
    for key, value in output.items():
        # Node
        pprint(f"Node '{key}':")
        # Optional: print full state at each node
        # pprint.pprint(value["keys"], indent=2, width=80, depth=None)
    pprint("\n---\n")

# Final generation
pprint(value['documents'])

def ask_question(user_question: str):
    """
    This function will be called by frontend
    """
    inputs = {
        "question": user_question
    }

    final_docs = None

    for output in app.stream(inputs):
        for key, value in output.items():
            final_docs = value

    # Handle wiki vs vectorstore output
    docs = final_docs["documents"]

    if isinstance(docs, list):
        return docs[0].page_content
    else:
        return docs.page_content

